{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/58/b0/034eb5d5ba12d66ab658ff3455a31f20add0b78df8203c6a7451bd1bee21/numpy-2.2.1-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Downloading numpy-2.2.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.1-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-2.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Input Documents\n",
    "documents = [\n",
    "    \"I like deep learning\",\n",
    "    \"I like NLP and deep learning\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the words that is contained by the document\n",
      "[['i', 'like', 'deep', 'learning'], ['i', 'like', 'nlp', 'and', 'deep', 'learning']]\n",
      "Unique words in the docs\n",
      "['and', 'deep', 'i', 'learning', 'like', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize the documents and create the vocabulary\n",
    "print(\"all the words that is contained by the document\")\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "print(tokenized_docs)\n",
    "vocabulary = sorted(set(word for doc in tokenized_docs for word in doc))\n",
    "print(\"Unique words in the docs\")\n",
    "print(vocabulary)\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to index mapping\n",
      "{'and': 0, 'deep': 1, 'i': 2, 'learning': 3, 'like': 4, 'nlp': 5}\n"
     ]
    }
   ],
   "source": [
    "# Map each word to an index\n",
    "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "print(\"Word to index mapping\")\n",
    "print(word_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Co-occurrence matrix\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create an empty co-occurrence matrix\n",
    "co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "print(\"Empty Co-occurrence matrix\")\n",
    "print(co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1: i, Word 2: i\n",
      "Word 1: i, Word 2: like\n",
      "Word 1: i, Word 2: deep\n",
      "Word 1: i, Word 2: learning\n",
      "Word 1: like, Word 2: i\n",
      "Word 1: like, Word 2: like\n",
      "Word 1: like, Word 2: deep\n",
      "Word 1: like, Word 2: learning\n",
      "Word 1: deep, Word 2: i\n",
      "Word 1: deep, Word 2: like\n",
      "Word 1: deep, Word 2: deep\n",
      "Word 1: deep, Word 2: learning\n",
      "Word 1: learning, Word 2: i\n",
      "Word 1: learning, Word 2: like\n",
      "Word 1: learning, Word 2: deep\n",
      "Word 1: learning, Word 2: learning\n",
      "Word 1: i, Word 2: i\n",
      "Word 1: i, Word 2: like\n",
      "Word 1: i, Word 2: nlp\n",
      "Word 1: i, Word 2: and\n",
      "Word 1: i, Word 2: deep\n",
      "Word 1: i, Word 2: learning\n",
      "Word 1: like, Word 2: i\n",
      "Word 1: like, Word 2: like\n",
      "Word 1: like, Word 2: nlp\n",
      "Word 1: like, Word 2: and\n",
      "Word 1: like, Word 2: deep\n",
      "Word 1: like, Word 2: learning\n",
      "Word 1: nlp, Word 2: i\n",
      "Word 1: nlp, Word 2: like\n",
      "Word 1: nlp, Word 2: nlp\n",
      "Word 1: nlp, Word 2: and\n",
      "Word 1: nlp, Word 2: deep\n",
      "Word 1: nlp, Word 2: learning\n",
      "Word 1: and, Word 2: i\n",
      "Word 1: and, Word 2: like\n",
      "Word 1: and, Word 2: nlp\n",
      "Word 1: and, Word 2: and\n",
      "Word 1: and, Word 2: deep\n",
      "Word 1: and, Word 2: learning\n",
      "Word 1: deep, Word 2: i\n",
      "Word 1: deep, Word 2: like\n",
      "Word 1: deep, Word 2: nlp\n",
      "Word 1: deep, Word 2: and\n",
      "Word 1: deep, Word 2: deep\n",
      "Word 1: deep, Word 2: learning\n",
      "Word 1: learning, Word 2: i\n",
      "Word 1: learning, Word 2: like\n",
      "Word 1: learning, Word 2: nlp\n",
      "Word 1: learning, Word 2: and\n",
      "Word 1: learning, Word 2: deep\n",
      "Word 1: learning, Word 2: learning\n",
      "Co-occurrence matrix\n",
      "[[4. 4. 4. 4. 4. 4.]\n",
      " [4. 8. 8. 8. 8. 4.]\n",
      " [4. 8. 8. 8. 8. 4.]\n",
      " [4. 8. 8. 8. 8. 4.]\n",
      " [4. 8. 8. 8. 8. 4.]\n",
      " [4. 4. 4. 4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Count co-occurrences\n",
    "for doc in tokenized_docs:\n",
    "    # Count words in the document\n",
    "    word_counts = Counter(doc)\n",
    "    \n",
    "    # Update the matrix for each pair of words\n",
    "    for (word1, word2) in product(doc, repeat=2):\n",
    "        print(f\"Word 1: {word1}, Word 2: {word2}\")\n",
    "        \n",
    "        i, j = word_to_index[word1], word_to_index[word2]\n",
    "        co_occurrence_matrix[i, j] += word_counts[word2]\n",
    "print(\"Co-occurrence matrix\")\n",
    "print(co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and', 'deep', 'i', 'learning', 'like', 'nlp']\n",
      "\n",
      "Co-occurrence Matrix (Raw Counts):\n",
      "[[4. 4. 4. 4. 4. 4.]\n",
      " [4. 8. 8. 8. 8. 4.]\n",
      " [4. 8. 8. 8. 8. 4.]\n",
      " [4. 8. 8. 8. 8. 4.]\n",
      " [4. 8. 8. 8. 8. 4.]\n",
      " [4. 4. 4. 4. 4. 4.]]\n",
      "\n",
      "Normalized Co-occurrence Matrix:\n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.1        0.2        0.2        0.2        0.2        0.1       ]\n",
      " [0.1        0.2        0.2        0.2        0.2        0.1       ]\n",
      " [0.1        0.2        0.2        0.2        0.2        0.1       ]\n",
      " [0.1        0.2        0.2        0.2        0.2        0.1       ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Normalize rows by row sum\n",
    "row_sums = co_occurrence_matrix.sum(axis=1, keepdims=True)\n",
    "normalized_matrix = co_occurrence_matrix / row_sums\n",
    "\n",
    "# Display results\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"\\nCo-occurrence Matrix (Raw Counts):\")\n",
    "print(co_occurrence_matrix)\n",
    "print(\"\\nNormalized Co-occurrence Matrix:\")\n",
    "print(normalized_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
